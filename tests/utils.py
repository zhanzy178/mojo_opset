import csv
import functools
import inspect
import os
import re
import sys
import time

from typing import Callable
from typing import Union
from typing import Tuple
from typing import Any

import pytest
import torch

try:
    import torch_npu
except Exception:
    torch_npu = None

from mojo_opset.utils.logging import get_logger
from mojo_opset.utils.platform import get_platform

logger = get_logger(__name__)


def assert_close(
    results: Union[torch.Tensor, Tuple[Any, ...]],
    refs: Union[torch.Tensor, Tuple[Any, ...]],
):
    assert type(results) is type(refs)
    if isinstance(results, torch.Tensor) and isinstance(refs, torch.Tensor):
        results = tuple([results])
        refs = tuple([refs])

    for result, ref in zip(results, refs):
        if isinstance(result, torch.Tensor) and isinstance(ref, torch.Tensor):
            assert result.shape == ref.shape
            assert result.dtype == ref.dtype
            dtype = result.dtype
            if dtype == torch.bfloat16:
                max_atol = 0.1
                max_rtol = 0.05
                mean_atol = 0.01
                mean_rtol = 0.01
            elif dtype == torch.float16:
                max_atol = 2e-2
                max_rtol = 2e-2
                mean_atol = 2e-2
                mean_rtol = 2e-2
            elif dtype == torch.float32:
                max_atol = 6e-3
                max_rtol = 6e-3
                mean_atol = 1e-4
                mean_rtol = 1e-4
            else:
                logger.warning(f"dtype {dtype} is not supported.")
                assert False

            torch.testing.assert_close(result.to(torch.float32), ref.to(torch.float32), atol=max_atol, rtol=max_rtol)
            assert (
                torch.mean(torch.abs(ref - result)) < max_atol
                or torch.mean(torch.abs((ref - result) / (ref + mean_atol))) < mean_rtol
            )
        else:
            assert result == ref


def get_executor_info(executor):
    def format_arg(arg):
        if isinstance(arg, torch.Tensor):
            return f"Tensor(shape={tuple(arg.shape)}, dtype={arg.dtype}, device={arg.device})"
        elif isinstance(arg, (int, float, str, bool)):
            return str(arg)
        elif isinstance(arg, (list, tuple)):
            return "[" + ", ".join(format_arg(a) for a in arg) + "]"
        elif arg is None:
            return "None"
        else:
            return f"<{type(arg).__name__}>"

    try:
        sig = inspect.signature(executor)
    except (TypeError, ValueError):
        logger.error("<Unknown callable>")
        return None

    if len(sig.parameters) > 0:
        logger.error("<Inputs unknown: executor takes parameters>")
        return None

    closure = executor.__closure__
    freevars = executor.__code__.co_freevars

    if not closure:
        logger.error("<No inputs (no closure)>")
        return None

    result = []
    for name, cell in zip(freevars, closure):
        value = cell.cell_contents
        result.append(f"{name}: {format_arg(value)}")

    matches = [re.search(r"<(.*?)>", r).group(1) for r in result if re.search(r"<(.*?)>", r) is not None]

    # Currently extracting class names from __closure__ using angle brackets.
    # TODO: Evaluate a more robust approach.
    assert len(matches) == 1
    func_name = matches[0]
    result = [r for r in result if f"<{func_name}>" not in r]

    # if "forward_ref" in inspect.getsource(executor).strip():
    #     func_name += "_TORCH_REF"

    return func_name, result


def format_executor_info(info_list):
    func = info_list[0]
    args = info_list[1:]

    arg_lines = "<br>  " + "<br>  ".join(args) if args else ""

    return f"{func}{arg_lines}"


def auto_switch_platform(set_perf: bool = False):
    device = get_platform()

    if set_perf:
        if device == "npu":
            perf_fn = perf_npu
        else:
            raise NotImplementedError(f"Performance test is not implemented on {device}")

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            new_args = [arg.to(device=device) if isinstance(arg, torch.Tensor) else arg for arg in args]
            new_kwargs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in kwargs.items()}

            if set_perf:
                module = sys.modules[func.__module__]
                setattr(module, "perf", perf_fn)

            return func(*new_args, **new_kwargs)

        return wrapper

    return decorator


# Skip current test if this case is not implemented on current chosen backend.
def bypass_not_implemented(func: Callable) -> Callable:
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except NotImplementedError:
            pytest.skip("Not implemented on this backend, skipped.")
            return None

    return wrapper


def is_similar(result, golden, atol=1e-2, rtol=1e-2):
    return result.shape == golden.shape and torch.allclose(result, golden, atol, rtol)


def get_max_diff(result, golden):
    return torch.max(torch.abs(result - golden))


def device_perf_npu(executor, profiling_dir="./npu_profiling", active=5):
    if not os.path.exists(profiling_dir):
        os.makedirs(profiling_dir)

    experimental_config = torch_npu.profiler._ExperimentalConfig(
        aic_metrics=torch_npu.profiler.AiCMetrics.PipeUtilization,
        profiler_level=torch_npu.profiler.ProfilerLevel.Level2,
        l2_cache=False,
        data_simplification=False,
    )
    # When using the Triton backend, JIT compilation causes the first execution to include
    # extra compile-time ops; without a warm-up run, the op counts in the profiling results
    # will be inconsistent.
    executor()
    torch.npu.synchronize()
    with torch_npu.profiler.profile(
        activities=[torch_npu.profiler.ProfilerActivity.CPU, torch_npu.profiler.ProfilerActivity.NPU],
        schedule=torch_npu.profiler.schedule(wait=0, warmup=5, active=active, repeat=1, skip_first=0),
        on_trace_ready=torch_npu.profiler.tensorboard_trace_handler(profiling_dir),
        record_shapes=True,
        profile_memory=True,
        with_stack=True,
        with_flops=False,
        with_modules=False,
        experimental_config=experimental_config,
    ) as prof:
        mat_a = torch.randn(4096, 4096).to(dtype=torch.bfloat16).npu()
        mat_b = torch.randn(4096, 4096).to(dtype=torch.bfloat16).npu()
        mat_c = torch.matmul(mat_a, mat_b)
        mat_c.cpu()

        for _ in range(10):
            executor()
            prof.step()

        torch.npu.synchronize()

    try:
        all_subdirs = [
            os.path.join(profiling_dir, d)
            for d in os.listdir(profiling_dir)
            if os.path.isdir(os.path.join(profiling_dir, d))
        ]

        if all_subdirs:
            return max(all_subdirs, key=os.path.getmtime)
        else:
            logger.warning("Profiling dir unfound.")
            return None

    except Exception as e:
        logger.warning(f"Failed to get Profiling folder name: {e}")
        return None


def host_perf(func, device, warmup=3, repeat=10):
    if device.lower() == "npu":
        sync = torch.npu.synchronize
    else:
        sync = lambda: None

    for _ in range(warmup):
        func()
        sync()

    start = time.time()
    for _ in range(repeat):
        func()
        sync()
    end = time.time()

    avg_time = (end - start) / repeat * 1000
    return avg_time


def perf_npu(executor, profiling_dir="./npu_profiling", active=5):
    kernel_profiling_path = device_perf_npu(executor, profiling_dir, active)
    csv_file_path = os.path.join(kernel_profiling_path, "ASCEND_PROFILER_OUTPUT", "op_statistic.csv")

    if not os.path.exists(csv_file_path):
        logger.warning(f"File not found: {csv_file_path}")
        return None

    total_avg_time_us = 0.0

    with open(csv_file_path, mode="r", newline="", encoding="utf-8") as file:
        reader = csv.DictReader(file)

        for row in reader:
            avg_time = float(row["Total Time(us)"])
            total_avg_time_us += avg_time

    device_latency = total_avg_time_us / active
    host_latency = host_perf(executor, "npu")

    func_name, para_list = get_executor_info(executor)

    plain_log_full = (
        f"[{func_name}] | "
        f"{', '.join(para_list)} | "
        f"Device latency = {device_latency:.4f} us | "
        f"Host latency = {host_latency:.4f} ms | "
        f"Profile dir = {kernel_profiling_path}"
    )

    logger.info(plain_log_full)

    plain_log_file = (
        f"| {func_name} | {format_executor_info(para_list)} | {device_latency:.4f} us | {host_latency:.4f} ms |"
    )
    log_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "perf/benchmark.md")
    os.makedirs(os.path.dirname(log_path), exist_ok=True)

    with open(log_path, "a", encoding="utf-8") as f:
        if not os.path.exists(log_path) or os.path.getsize(log_path) == 0:
            f.write("| Name | Parameters | Device Latency (us) | Host Latency (ms) |\n")
            f.write("|------|------------|---------------------|-------------------|\n")
        f.write(plain_log_file + "\n")


class MockFunctionCtx:
    def __init__(self):
        self.saved_tensors = None

    def save_for_backward(self, *tensors):
        self.saved_tensors = tensors
