# Mojo Opset
## 1. 简介
Mojo Opset 是一个基于面向 LLM & DiT 类模型专用 Opset，支持多种硬件加速器以及不同的算子实现。用户能够基于 Mojo Opset
快速搭建 LLM 模型，轻松获取不同硬件加速器的 SOTA 性能。Mojo Opset 包含推理加速和训练加速两部分算子，其中推理部分以
Mojo Operator方式提供，训练部分则以 Mojo Function部分提供。


## 2. 实现后端

### 2.1 ttx-kernels
ttx-kernels 提供了 Mojo Opset 的 triton 版本实现。

source code: mojo_opset/backends/ttx/kernels

### 2.2 torch_npu(ongoing)
Ascend NPU官方支持。


## 3. Op List

### 3.1 Mojo Operator List

| Op Category | Op Name                     | torch ref         | triton implement |
| :---------- | :-------------------------- | :---------------- | :------------ |
| Embedding   | MojoEmbedding               | TBD               | TBD           |
| Embedding   | MojoParallelEmbedding       | TBD               | TBD           |
| Attention   | MojoPagedPrefillGQA         | ✅                | ✅             |
| Attention   | MojoPagedDecodeGQA          | ✅                | ✅             |
| Attention   | MojoPagedPrefillMLA         | TBD               | TBD           |
| Attention   | MojoPagedDecodeMLA          | TBD               | TBD           |
| Attention   | MojoPagedPrefillNSA         | TBD               | TBD           |
| Attention   | MojoPagedDecodeNSA          | TBD               | TBD           |
| Attention   | MojoWindownAttenton         | TBD               | TBD           |
| MoE         | MojoMoEGate                 | ✅                | TBD           |
| MoE         | MojoMoEDispatch             | ✅                | TBD           |
| MoE         | MojoMoECombine              | ✅                | TBD           |
| MoE         | MojoMoeDispatchQuant        | TBD               | TBD           |
| Sampling    | MojoTopKSampling            | TBD               | TBD           |
| Sampling    | MojoTopPSampling            | ✅                | ✅             |
| Sampling    | MojoTopPSampling            | ✅                | ✅             |
| Sampling    | MojoRejectSampling          | ✅                | ✅             |
| Sampling    | MojoApplyPenaltiesTempurate | ✅                | ✅             |
| Norm        | MojoNorm                    | ✅                | ✅             |
| Norm        | MojoResidualAddNorm         | ✅                | ✅             |
| Norm        | MojoNormQuant               | TBD               | TBD           |
| Norm        | MojoResidualAddNormQuant    | TBD               | TBD           |
| Norm        | MojoResidualAddNormCast     | TBD               | TBD           |
| PositionEmb | MojoRotaryEmb               | ✅                | ✅             |
| PositionEmb | MojoNormRotary              | TBD               | TBD           |
| PositionEmb | MojoNormRotaryStorKV        | TBD               | TBD           |
| KVCache     | MojoKVCacheCast             | TBD               | TBD           |
| KVCache     | MojoStorePagedKVCache       | ✅                | TBD           |
| KVCache     | MojoStorePagedMLAKVCache    | TBD               | TBD           |
| Linear      | MojoLinear                  | ✅                | TBD           |
| Linear      | MojoQuantLinear             | TBD               | TBD           |
| Linear      | MojoBatchLinear             | TBD               | TBD           |
| Linear      | MojoGroupLinear             | ✅                | TBD           |
| Quantize    | MojoQuant                   | TBD               | TBD           |
| Quantize    | MojoDequant                 | TBD               | TBD           |
| Activation  | MojoGelu                    | ✅                | ✅             |
| Activation  | MojoSilu                    | ✅                | ✅             |
| Activation  | MojoSwiGlu                  | ✅                | ✅             |
| Activation  | MojoSiluQuant               | TBD               | TBD           |
| Activation  | MojoGeluQuant               | TBD               | TBD           |
| Activation  | MojoSwiGluQuant             | TBD               | TBD           |
| Comm&Comp   | MojoLinearAllReduce         | TBD               | TBD           |
| Comm&Comp   | MojoAllGatherLinear         | TBD               | TBD           |
| Comm&Comp   | MojoLinearAll2All           | TBD               | TBD           |
| Comm&Comp   | MojoLinearReduceScatter     | TBD               | TBD           |


### 3.2 Mojo Function List

| Op Category | Op Name                     | Description       | Additional    |
| :---------- | :-------------------------- | :---------------- | :------------ |
| Attention   | MojoFlashAttentionFunc      | TBD               | TBD           |
| PositionEmb | MojoRotaryEmbFunc           | ✅                | ✅             |
| Activation  | MojoSiluFunc                | ✅                | ✅             |
| Activation  | MojoSwiGluFunc              | TBD               | TBD           |
| MoE         | MojoMoEGatingFunc           | TBD               | TBD           |
| Norm        | MojoRMSNormFunc             | ✅                | ✅             |
| Comm&Comp   | MojoLinearAllReduce         | TBD               | TBD           |
| Loss        | MojoLinearCrossEntropyFunc  | ✅                | ✅             |


## 4. Usage
### 4.1 apply mojo op
```python
from mojo_opset import MojoSilu

silu = MojoSilu(
    op_name="demo",
    layer_idx=0,
)

silu(torch.randn(128, 128).npu())
```

### 4.2 backend selection
您可以通过环境变量`MOJO_BACKEND`来控制您想要选用的后端，当前支持的后端主要为`TTX`；当您添加多个后端后，
Mojo Opset 会按照内部的优先级顺序来选用后端实现（后续我们将添加一个 tuner 功能，自动选取当前场景下的最优实现）。
默认会开启所有后端，即`+ALL`。
```bash
export MOJO_BACKEND="+TTX"
```

### 4.3 modeling ref
以 qwen3 dense 为例 [modify from here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py)，您可以通过以下任意一种方式使用 Mojo Opset 构建模型：

(1) monkey patch

modeling/torch_qwen3_dense.py 中提供了原生 torch 实现的 modeling，我们实现了相应的 monkey-patch 替换机制（mojo_opset/mojo_monkey_patch.py），仅需一行代码即可将 native modeling 中若干组件替换为 Mojo op，并进一步 dispatch 到高性能后端实现。您可以运行：
```bash
MOJO_BACKEND="+TTX" pytest -s tests/test_qwen3_dense_patching.py
```
跑通一个 decoder layer 的 prefill/decode 流程。

(2) 即插即用

modeling/mojo_qwen3_dense.py 中提供了直接基于 Mojo Opset 实现的 modeling，效果等同于(1)中 monkey-patch 替换后的模型。

### 4.4 compatibility with torch.compile
您可以通过环境变量`MOJO_RUN_MODE`来控制您想要选用的运行模式，当前支持的运行模式包括`EAGER`, `COMPILE`；默认会开启`EAGER`模式。
其中`COMPILE`模式要求当前torch版本>=2.7.0，否则会报错。
```bash
# 如果你希望当前triton kernel被注册到torch.library中，并支持被torch.dynamo捕获，以支持更长远的优化（默认模式）。
export MOJO_RUN_MODE="COMPILE"

# 如果你希望当前triton kernel被直接调用，而不是被注册到torch.library中（该方式在eager模式下能轻微减少torch的overhead）。
export MOJO_RUN_MODE="EAGER"
```

### 4.5 E2E model generation example for Qwen3-8B
```bash
# 使用默认逻辑（自动下载到 ./Qwen3-8B）
./examples/run_model.sh

# 指定自定义路径
./examples/run_model.sh /path/to/your/model

# 期望输出
Weight Loading Report:
  Total Expected Keys: 399
  Successfully Loaded: 399
  Missing Keys: 0
  Unexpected Keys: 0
Loading tokenizer...

Prompt: 你好，请介绍一下你自己。
----------------------------------------
----------------------------------------
Generated text:  你好！我是一个大型语言模型，名叫通义千问，由通义实验室研发。我能够进行多轮对话，回答各种问题，创作文字，比如写故事、写邮件、写剧本等，还能进行逻辑推理、表达观点，甚至编写和调试程序。我的训练数据来自于互联网上的大量文本，因此我具备广泛的知识和语言理解能力。我可以用多种语言与你交流，包括中文、英文、日文、韩文等。
```