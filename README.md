# ğŸ§± Mojo Opset
## Overview
Mojo Opset is a domain specialized opset for LLMs and multimodal models that provides operator suites for both inference acceleration and training acceleration. It supports multiple hardware accelerators and diverse operator implementations, while abstracting away the differences and complexity of implementation strategies and hardware backends for users. The goal is to help users quickly build LLM models with Mojo Opset and achieve state-of-the-art performance across different accelerators.


## Backend Implementations

### Torch native
Mojo Opset provides a baseline implementation built on PyTorch native ops. This implementation serves as the golden reference for different backends and also functions as the fallback backend while other backends are being developed.

### ğŸ”¥ğŸ”¥ğŸ”¥ Triton-x (TTX for short)
TTX is a triton implementation for Mojo Opset.

Supported Hardware:
- Ascend NPU 910B/C

TTX now is compatible with `torch.compile`.
You can control the run mode via the `MOJO_RUN_MODE` environment variable. The supported modes are `EAGER` and `COMPILE`; `EAGER` is enabled by default. The `COMPILE` mode requires the current Torch version to be >= 2.7.0; otherwise, an error will be raised.
```bash
# If you want the current Triton kernel to be registered in torch.library and captured by torch.dynamo
# to enable longer-term optimizations (default mode).
export MOJO_RUN_MODE="COMPILE"

# If you want the current Triton kernel to be invoked directly rather than registered in torch.library
# (this can slightly reduce PyTorch overhead in eager mode).
export MOJO_RUN_MODE="EAGER"
```

source code: mojo_opset/backends/ttx/kernels

### Backend Selection
You can control the backend you want to use via the `MOJO_BACKEND` environment variable; the currently supported backends are list as below:
- "ttx"
- "torch"

When multiple backends are added, Mojo Opset selects the backend implementation according to its internal priority order (We plan to add a tuner feature later to automatically choose the optimal implementation for the current scenario).


## Op List

### Mojo Operator List

| Op Category | Op Name                     | torch native      | ttx           |
| :---------- | :-------------------------- | :---------------- | :------------ |
| Embedding   | MojoEmbedding               | TBD               | TBD           |
| Embedding   | MojoParallelEmbedding       | TBD               | TBD           |
| Attention   | MojoPagedPrefillGQA         | âœ…                | âœ…             |
| Attention   | MojoPagedDecodeGQA          | âœ…                | âœ…             |
| Attention   | MojoPagedPrefillMLA         | TBD               | TBD           |
| Attention   | MojoPagedDecodeMLA          | TBD               | TBD           |
| Attention   | MojoPagedPrefillNSA         | TBD               | TBD           |
| Attention   | MojoPagedDecodeNSA          | TBD               | TBD           |
| Attention   | MojoSlidingWindownAttenton  | TBD               | TBD           |
| Attention   | MojoSdpa                    | âœ…                | âœ…             |
| MoE         | MojoMoEGate                 | âœ…                | TBD           |
| MoE         | MojoMoEDispatch             | âœ…                | TBD           |
| MoE         | MojoMoECombine              | âœ…                | TBD           |
| MoE         | MojoMoeDispatchQuant        | TBD               | TBD           |
| Sampling    | MojoTopKSampling            | TBD               | TBD           |
| Sampling    | MojoTopPSampling            | âœ…                | âœ…             |
| Sampling    | MojoTopPSampling            | âœ…                | âœ…             |
| Sampling    | MojoRejectSampling          | âœ…                | âœ…             |
| Sampling    | MojoApplyPenaltiesTempurate | âœ…                | âœ…             |
| Norm        | MojoNorm                    | âœ…                | âœ…             |
| Norm        | MojoResidualAddNorm         | âœ…                | âœ…             |
| Norm        | MojoNormQuant               | TBD               | TBD           |
| Norm        | MojoResidualAddNormQuant    | TBD               | TBD           |
| Norm        | MojoResidualAddNormCast     | TBD               | TBD           |
| PositionEmb | MojoRotaryEmb               | âœ…                | âœ…             |
| PositionEmb | MojoNormRotary              | TBD               | TBD           |
| PositionEmb | MojoNormRotaryStorKV        | TBD               | TBD           |
| KVCache     | MojoKVCacheCast             | TBD               | TBD           |
| KVCache     | MojoStorePagedKVCache       | âœ…                | âœ…             |
| KVCache     | MojoStorePagedMLAKVCache    | TBD               | TBD           |
| Linear      | MojoLinear                  | âœ…                | TBD           |
| Linear      | MojoQuantLinear             | TBD               | TBD           |
| Linear      | MojoBatchLinear             | TBD               | TBD           |
| Linear      | MojoGroupLinear             | âœ…                | âœ…             |
| Quantize    | MojoQuant                   | TBD               | TBD           |
| Quantize    | MojoDequant                 | TBD               | TBD           |
| Activation  | MojoGelu                    | âœ…                | âœ…             |
| Activation  | MojoSilu                    | âœ…                | âœ…             |
| Activation  | MojoSwiGlu                  | âœ…                | âœ…             |
| Activation  | MojoSiluQuant               | TBD               | TBD           |
| Activation  | MojoGeluQuant               | TBD               | TBD           |
| Activation  | MojoSwiGluQuant             | TBD               | TBD           |
| Comm&Comp   | MojoLinearAllReduce         | TBD               | TBD           |
| Comm&Comp   | MojoAllGatherLinear         | TBD               | TBD           |
| Comm&Comp   | MojoLinearAll2All           | TBD               | TBD           |
| Comm&Comp   | MojoLinearReduceScatter     | TBD               | TBD           |


### Mojo Function List

| Op Category | Op Name                     | torch native      | ttx           |
| :---------- | :-------------------------- | :---------------- | :------------ |
| Attention   | MojoSdpaFunc                | âœ…                | âœ…             |
| Attention   | MojoDiffusionAttentionFunc  | âœ…                | âœ…             |
| PositionEmb | MojoRotaryEmbFunc           | âœ…                | âœ…             |
| Activation  | MojoSiluFunc                | âœ…                | âœ…             |
| Activation  | MojoSwiGluFunc              | TBD               | TBD           |
| MoE         | MojoMoEGatingFunc           | TBD               | TBD           |
| Norm        | MojoRMSNormFunc             | âœ…                | âœ…             |
| Comm&Comp   | MojoLinearAllReduce         | TBD               | TBD           |
| Loss        | MojoLinearCrossEntropyFunc  | âœ…                | âœ…             |


## Usage
### Apply mojo op
```python
from mojo_opset import MojoSilu

silu = MojoSilu()

silu(torch.randn(128, 128))
```

### Modeling with Mojo Opset
You can build the model using Mojo Opset in the following ways:

1. Build model from mojo opset

    You can also build your modeling by mojo opset directly, [Mojo qwen3 dense modeling](./mojo_opset/modeling/mojo_qwen3_dense.py) is an example.

2. Patch for transformers models(ğŸš§ coming soon).

    For [hugging face transformers](https://github.com/huggingface/transformers) models, you can use Mojo Opset to build the model by monkey patching the original modeling code.

    ```python
    from transformers import Qwen3ForCausalLM

    # 1. Apply mojo opset to qwen3 model
    mojo_opset.patching.apply_mojo_to_qwen3()

    
    # 2. Instantiate patched model
    model = transformers.AutoModelForCausalLM("path/to/qwen3/model")
    ```


### E2E model generation example for Qwen3-8B
```bash
bash ./examples/run_model.sh

Prompt: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚
----------------------------------------
----------------------------------------
Generated text:  ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåå«é€šä¹‰åƒé—®ï¼Œç”±é€šä¹‰å®éªŒå®¤ç ”å‘ã€‚æˆ‘èƒ½å¤Ÿè¿›è¡Œå¤šè½®å¯¹è¯ï¼Œå›ç­”å„ç§é—®é¢˜ï¼Œåˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€è¡¨è¾¾è§‚ç‚¹ï¼Œç”šè‡³ç¼–å†™å’Œè°ƒè¯•ç¨‹åºã€‚æˆ‘çš„è®­ç»ƒæ•°æ®æ¥è‡ªäºäº’è”ç½‘ä¸Šçš„å¤§é‡æ–‡æœ¬ï¼Œå› æ­¤æˆ‘å…·å¤‡å¹¿æ³›çš„çŸ¥è¯†å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚æˆ‘å¯ä»¥ç”¨å¤šç§è¯­è¨€ä¸ä½ äº¤æµï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ç­‰ã€‚
```

## ğŸš§ Future Work
- Add more mojo ops.
- Support more backend implementations and support more Hardware accelerators.
    - Ascend NPU's official implementation using Ascend C language.
    - Support Cambircon MLU using triton language.
- Performance optimization.
    - A tuner for various backend implementations, ensure users can always get the best performance.
    - A compilation mechanism for replacement the original torch ops with mojo ops.